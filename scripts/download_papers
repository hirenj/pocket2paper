#!/bin/bash

SCIHUBPATH=$HOME/dev/scihub.py
outdir=$(pwd)

urlencode() {
    # urlencode <string>
    old_lc_collate=$LC_COLLATE
    LC_COLLATE=C

    local length="${#1}"
    for (( i = 0; i < length; i++ )); do
        local c="${1:i:1}"
        case $c in
            [a-zA-Z0-9.~_-]) printf "$c" ;;
            *) printf '%%%02X' "'$c" ;;
        esac
    done

    LC_COLLATE=$old_lc_collate
}

retrieve_pmid() {
	curl -s "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&field=aid&retmode=json&term=$1" | json esearchresult.idlist[0]
}

retrieve_biorxiv() {
	if [[ $1 =~ 10\.1101 ]]; then
		echo "biorxiv-$1" | sed -e 's/10\.1101\///';
	fi
}

retrieve_chemrxiv() {
	if [[ $1 =~ 10\.26434 ]]; then
		echo "chemrxiv-$1" | sed -e 's/10\.26434\///';
	fi
}

retrieve_peerj() {
	if [[ $1 =~ 10\.7717 ]]; then
		echo "peerj-$1" | sed -e 's/10\.7717\///';
	fi
}

retrieve_author_info() {
	doi="$1"
	info_json=$(curl -s "https://api.crossref.org/works/$doi")
	pubdate=$(echo "$info_json" | json message.issued.date-parts[0][0])
	pubauthor=$(echo "$info_json" | json message.author[0].family | tr -d ' ' | iconv -f utf-8 -t ascii//translit | tr -d '"' | tr -d "'")
	echo "$pubauthor$pubdate"
}

retrieve_identifier() {
	identifier=$(retrieve_pmid $1)
	if [ -z "$identifier" ]; then
		identifier=$(retrieve_biorxiv $1)
	fi
	if [ -z "$identifier" ]; then
		identifier=$(retrieve_chemrxiv $1)
	fi
	if [ -z "$identifier" ]; then
		identifier=$(retrieve_peerj $1)
	fi
	if [ -z "$identifier" ]; then
		identifier='XXXX'
	fi
	echo "$identifier"
}

retrieve_filename() {
	doi="$1"
	identifier=$(retrieve_identifier $doi)
	name=$(retrieve_author_info $doi)
	echo "$name-$identifier"
}

rewrite_chemrxiv() {
	chemrxiv_url="$1"
# https://chemrxiv.org/articles/A_Mucin-Specific_Protease_Enables_Molecular_and_Functional_Analysis_of_Human_Cancer-Associated_Mucins/7330676/1
# https://ndownloader.figshare.com/articles/7330676/versions/1/export_pdf
	url_parts=$(echo "$chemrxiv_url" | sed -e 's/.*\/\([0-9][0-9]*\)\/\([0-9][0-9]*\)/\1	\2/')
	articleid="${url_parts%$'\t'*}"
	version="${url_parts#*$'\t'}"
	echo "https://ndownloader.figshare.com/articles/${articleid}/versions/${version}/export_pdf"
}


while IFS='$\n' read -r doi; do
	if [[ -z ${doi// } ]]; then
		continue
	fi

	tmpdir=`mktemp -d 2>/dev/null || mktemp -d -t 'scihub'`
	outfile=$(retrieve_filename "$doi")

	echo "$doi $outfile"

	if [[ -e "$outdir/$outfile.pdf" ]]; then
		continue
	fi

	if [[ $outfile =~ biorxiv ]]; then
		new_url=$(curl -Ls -o /dev/null -w %{url_effective} "http://dx.doi.org/$doi")
		curl -Ls -o $tmpdir/$outfile.pdf "$new_url.full.pdf"
	elif [[ $outfile =~ chemrxiv ]]; then
		new_url=$(curl -Ls -o /dev/null -w %{url_effective} "http://dx.doi.org/$doi")
		chemrxiv_url=$(rewrite_chemrxiv $new_url)
		curl -Ls -o $tmpdir/$outfile.pdf "$chemrxiv_url"
	elif [[ $doi =~ 10\.7717 ]]; then
		new_url=$(curl -Ls -o /dev/null -w %{url_effective} "http://dx.doi.org/$doi" | sed -e 's/\/$//')
		curl -Ls -o $tmpdir/$outfile.pdf "$new_url.pdf"
	else
		python2.7 $SCIHUBPATH/scihub/scihub.py -d $doi -o $tmpdir
		if [ $? -gt 0 ]; then
			echo "Error getting DOI $doi $outdir"
			continue
		fi
	fi

	for pdf in $tmpdir/*; do mv -v -- "$pdf" "$outdir/$outfile.pdf"; break; done

done

